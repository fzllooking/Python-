# Xpath
```PYTHON
xpath使用:
注意:提前安装xpath插件(在谷歌商店中下载xpath helper)
快捷方式:ctrl+shift+x
1.安装lxml库
  pip install  -i https://pypi.douban.com/simple lxml
2.导入lxml.etree
  from lxml import etree
3.etree.parse() 解析本地文件
  html_tree = etree.parse('xx.html')
4.etree.HTML() 服务器响应文件
  html_tree = etree.HTML(response.read().decode('utf-8'))
5.html_tree.xpath(xpath路径)
# xpath路径从前端网页中获取，一层一层往下走即可.

```
## xpath基本语法：
当前有一个html文件：
```HTML
<!DOCTYPE html>
<html lang="en">
<head>
<!--utf-8后面要加/，代表结束-->
    <meta charset="UTF-8"/>
    <title>Title</title>
</head>
<body>
    <ul>
        <li id="l1" class="c1">北京</li>
        <li>上海</li>
        <li>深圳</li>
        <li>武汉</li>
    </ul>

    <ul>
        <li>南昌</li>
        <li>长沙</li>
        <li>中国</li>
    </ul>
</body>
</html>
```
* 1.路径查询
```PYTHON
//:查找所有子孙节点，不考虑层级关系
/:找直接子节点
* 例如：
```python
from lxml import etree

# xpath解析本地文件
tree = etree.parse('xpath.html')

# 查找ul下面的li
li_list = tree.xpath('//ul/li')

# 或者:li_list = tree.xpath('//body//li')

print(tree)
print(li_list)
print(len(li_list))

# <lxml.etree._ElementTree object at 0x000001BD3325FF40>
# [<Element li at 0x1bd33664600>, <Element li at 0x1bd33664640>, <Element li at 0x1bd33664680>, <Element li at 0x1bd336646c0>, <Element li at 0x1bd33664700>, <Element li at 0x1bd33664780>, <Element li at 0x1bd336647c0>]
7
```


* 2.谓词查询
 ```PYTHON
//div[@id]
//div[@id="maincontent"]

* 例如，查询带有id的城市
from lxml import etree
tree = etree.parse('xpath.html')
# text()获取标签内的内容
li_list = tree.xpath('//ul/li[@id]/text()')

print(li_list)
print(len(li_list))

# ['北京']
# 1

* 又例如,查询id=l1的城市
from lxml import etree
tree = etree.parse('xpath.html')
# text()获取标签内的内容
# 注意，单引号中，字符串要加双引号
li_list = tree.xpath('//ul/li[@id="l1"]/text()')

print(li_list)
# ['北京']
```
* 3.属性查询
```PYTHON
//@class

* 例如：查找id为l1的class属性值
li = tree.xpath('//ul/li[@id="l1"]/@class')

# ['c1']
```

* 4.模糊查询
```PYTHON
//div[contains(@id,"he")]
//div[starts-with(@id, "he")]

* 例如，id里包含l的li标签
li_list = tree.xpath('//ul/li[contains(@id,"l")]/text()')
```

* 5.内容查询
```PYTHON
//div/h1/text()
```

* 实例:爬取网站中的图片数据
* 如图，查找相应的图片数据:
![QQ20240729-114037](https://github.com/user-attachments/assets/f9bc87d3-c09e-4abf-a9d5-fd207b02f102)

* 这里通过class属性进行查找,将其下属所包含的所有数据全部提出：
```PYTHON
  name_list = tree.xpath('//img[@class="lazy"]/@alt')
  src_list = tree.xpath('//img[@class="lazy"]/@data-original')
```
* 其中,name_list为：

<img width="1264" alt="Snipaste_2024-07-29_11-52-12" src="https://github.com/user-attachments/assets/a9c3bfc2-e613-4111-83e6-b07b6e092880">

* 其中,src_list为：

<img width="1267" alt="Snipaste_2024-07-29_11-52-50" src="https://github.com/user-attachments/assets/374df094-5bfe-4f59-b148-633ea90eb51e">

```PYTHON

# 第一页：https://sc.chinaz.com/tupian/index.html
# 第二页：https://sc.chinaz.com/tupian/index_2.html
import urllib.request
from lxml import etree
def create_request(page):
    if(page == 1):
        url = 'https://sc.chinaz.com/tupian/index.html'
    else:
        url ='https://sc.chinaz.com/tupian/index_' + str(page) + '.html'
    headers = {
        'User-Agent': '......'
    }
    request = urllib.request.Request(url=url, headers=headers)
    return request

# 首先获得网页源码,进而通过xpath下载相应的数据图片
def get_content(request):
    response = urllib.request.urlopen(request)
    content = response.read().decode('utf-8')
    return content

# 下载图片
def down_load(content):
# urllib.request.urlretrieve('图片地址'，'文件的名字”)
    tree = etree.HTML(content)
# 通过class属性进行查找,将其下属所包含的所有数据全部提出
    name_list = tree.xpath('//img[@class="lazy"]/@alt')
# old_src_list = tree.xpath('//img[@class="lazy"]/@alt') src可能有懒加载，不是真实地址
    src_list = tree.xpath('//img[@class="lazy"]/@data-original')

# print(len(name_list),len(old_src_list))
# 结果为：40,0

# 逐步将图片输出出来
    for i in range(len(name_list)):
        name = name_list[i]
        src = src_list[i]
# 因为src没有自带https前缀，所以这里加上'https'
        url = 'https:' + src
# print(name, url)
# 如果不放在目录里,就是urllib.request.urlretrieve(url=url,filename= name + '.jpg')
        urllib.request.urlretrieve(url=url,filename='./tupian/'+ name + '.jpg')

if __name__ == '__main__':
    start_page=int(input("输入起始页码"))
    end_page=int(input("输入结束页码"))

    for page in range(start_page, end_page+1):
        request = create_request(page)
        content = get_content(request)
        down_load(content)
```
* 结果如图：

![image](https://github.com/user-attachments/assets/09f68a37-1ec6-4cfd-b787-33705cc81804)
